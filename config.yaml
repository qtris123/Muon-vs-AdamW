# Configuration for Catastrophic Forgetting Study
# Two-stage SFT pipeline: GSM8K -> AQUA-RAT

# Model Configuration
model:
  name: "meta-llama/Llama-3.2-1B"  # Default accessible model
  # name: "meta-llama/Llama-3.1-1B"  # Uncomment if you have access
  use_lora: true
  lora_rank: 16
  lora_alpha: 32
  lora_dropout: 0.1
  target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]

# Training Configuration
training:
  # Stage 1: GSM8K
  stage1:
    dataset: "gsm8k"
    epochs: 3
    batch_size: 4
    gradient_accumulation_steps: 4
    learning_rate: 2e-4
    weight_decay: 0.01
    warmup_steps: 100
    max_length: 1024
  
  # Stage 2: AQUA-RAT
  stage2:
    dataset: "aqua_rat"
    epochs: 3
    batch_size: 4
    gradient_accumulation_steps: 4
    learning_rate: 2e-4
    weight_decay: 0.01
    warmup_steps: 100
    max_length: 1024

# Optimizer Configuration
optimizer:
  type: "adamw"  # Options: "adamw", "muon"
  adamw:
    beta1: 0.9
    beta2: 0.999
    eps: 1e-8
  muon:
    beta1: 0.9
    beta2: 0.999
    eps: 1e-8
    mu: 0.01  # Muon-specific parameter

# Evaluation Configuration
evaluation:
  batch_size: 8
  max_new_tokens: 512
  temperature: 0.1
  do_sample: true
  pad_token_id: null  # Will be set automatically
  replay_samples: 100  # Number of samples to record for replay analysis

# Logging Configuration
logging:
  log_dir: "./logs"
  tensorboard_dir: "./tensorboard_logs"
  save_dir: "./checkpoints"
  log_interval: 10
  eval_interval: 100
  save_interval: 500

# Device Configuration
device: "auto"  # Options: "auto", "cuda", "cpu"
mixed_precision: true
